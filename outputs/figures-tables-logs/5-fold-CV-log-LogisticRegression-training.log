import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, f1_score
from sklearn.preprocessing import LabelEncoder
import joblib

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# === Step 1: Load Dataset ===
df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/full_dataset2.csv')
df = df[['TEXT', 'TARGET']].dropna()

# === Step 2: Encode Labels ===
le = LabelEncoder()
df['label_encoded'] = le.fit_transform(df['TARGET'])

# === Step 3: Split into 80% train_val and 20% test ===
train_val_df, test_df = train_test_split(
    df, test_size=0.2, stratify=df['label_encoded'], random_state=42
)

# === Step 4: Vectorize ===
vectorizer = TfidfVectorizer(max_features=5000)
X_train_val = vectorizer.fit_transform(train_val_df['TEXT'])
y_train_val = train_val_df['label_encoded']

X_test = vectorizer.transform(test_df['TEXT'])
y_test = test_df['label_encoded']

# === Step 5: K-Fold CV and Save Best Model ===
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
f1_scores = []
models = []
best_f1 = 0
best_model = None

print("Cross-validation performance (on 80% training data):")
for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_val, y_train_val), 1):
    X_train, X_val = X_train_val[train_idx], X_train_val[val_idx]
    y_train, y_val = y_train_val.iloc[train_idx], y_train_val.iloc[val_idx]

    clf = LogisticRegression(class_weight={0: 1, 1: 1}, max_iter=1000)
    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_val)
    f1 = f1_score(y_val, y_pred, average='macro')
    f1_scores.append(f1)
    models.append(clf)

    print(f" Fold {fold}: F1 Macro = {f1:.4f}")

    if f1 > best_f1:
        best_f1 = f1
        best_model = clf
        joblib.dump(clf, f'/content/drive/My Drive/Colab Notebooks/best_logreg_model_fold{fold}.joblib')

print(f"\nBest F1 Macro from CV: {best_f1:.4f}")

# === Step 6: Use best model on 20% holdout ===
y_pred_test = best_model.predict(X_test)
print("\nFinal Evaluation on 20% Holdout Set:")
print(classification_report(y_test, y_pred_test, target_names=le.classes_))

from sklearn.metrics import matthews_corrcoef, roc_auc_score

# Additional metrics: MCC and ROCâ€“AUC
mcc = matthews_corrcoef(y_test, y_test_pred)

# Use decision_function if available (LinearSVC); fallback to predict_proba if model supports it
y_test_score = None
if hasattr(best_model, "decision_function"):
    y_test_score = best_model.decision_function(X_test)
    # If decision_function returns (n_samples, n_classes), take the positive class column (1)
    if hasattr(y_test_score, "ndim") and y_test_score.ndim == 2:
        y_test_score = y_test_score[:, 1]
elif hasattr(best_model, "predict_proba"):
    y_test_score = best_model.predict_proba(X_test)[:, 1]

print("\nAdditional Metrics LogisticRegression:")




Cross-validation performance (on 80% training data):
 Fold 1: F1 Macro = 0.7715
 Fold 2: F1 Macro = 0.8181
 Fold 3: F1 Macro = 0.7723
 Fold 4: F1 Macro = 0.8191
 Fold 5: F1 Macro = 0.8175

Best F1 Macro from CV: 0.8191

Final Evaluation on 20% Holdout Set:
               precision    recall  f1-score   support

Non-offensive       0.80      0.83      0.81       100
    Offensive       0.82      0.78      0.80        96

     accuracy                           0.81       196
    macro avg       0.81      0.81      0.81       196
 weighted avg       0.81      0.81      0.81       196

