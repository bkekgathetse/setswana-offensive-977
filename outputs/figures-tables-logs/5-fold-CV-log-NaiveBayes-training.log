import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import f1_score, classification_report
from sklearn.preprocessing import LabelEncoder
import joblib

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Load dataset
df = pd.read_csv("/content/drive/My Drive/Colab Notebooks/full_dataset2.csv")
df = df[['TEXT', 'TARGET']]

# Encode labels
le = LabelEncoder()
df['label_encoded'] = le.fit_transform(df['TARGET'])

# Split into 80% train+val and 20% holdout test set
train_val_df, test_df = train_test_split(
    df, test_size=0.2, stratify=df['label_encoded'], random_state=42
)

# TF-IDF vectorization
vectorizer = TfidfVectorizer(max_features=5000)
X_train_val = vectorizer.fit_transform(train_val_df['TEXT'])
y_train_val = train_val_df['label_encoded'].values
X_test = vectorizer.transform(test_df['TEXT'])
y_test = test_df['label_encoded'].values

# Cross-validation on 80% to select best model
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
best_f1 = 0
best_model = None

for fold, (train_index, val_index) in enumerate(kf.split(X_train_val, y_train_val)):
    X_train, X_val = X_train_val[train_index], X_train_val[val_index]
    y_train, y_val = y_train_val[train_index], y_train_val[val_index]

    model = MultinomialNB()
    model.fit(X_train, y_train)
    y_val_pred = model.predict(X_val)
    f1 = f1_score(y_val, y_val_pred, average='macro')

    print(f"Fold {fold+1} F1 Score: {f1:.4f}")

    if f1 > best_f1:
        best_f1 = f1
        best_model = model
        joblib.dump(best_model, "/content/drive/My Drive/Colab Notebooks/best_nb_model.joblib")

# Load best model and evaluate on final holdout test set
best_model = joblib.load("/content/drive/My Drive/Colab Notebooks/best_nb_model.joblib")
y_pred = best_model.predict(X_test)

# Print evaluation
print("\nFinal Evaluation on 20% Holdout Test Set")
print(classification_report(y_test, y_pred, target_names=le.classes_))

from sklearn.metrics import matthews_corrcoef, roc_auc_score

# Additional metrics: MCC and ROC–AUC
mcc = matthews_corrcoef(y_test, y_test_pred)

# Use decision_function if available (LinearSVC); fallback to predict_proba if model supports it
y_test_score = None
if hasattr(best_model, "decision_function"):
    y_test_score = best_model.decision_function(X_test)
    # If decision_function returns (n_samples, n_classes), take the positive class column (1)
    if hasattr(y_test_score, "ndim") and y_test_score.ndim == 2:
        y_test_score = y_test_score[:, 1]
elif hasattr(best_model, "predict_proba"):
    y_test_score = best_model.predict_proba(X_test)[:, 1]

print("\nAdditional Metrics MultinomialNB:")
print(f"Matthews Correlation Coefficient (MCC): {mcc:.4f}")

if y_test_score is not None:
    try:
        roc_auc = roc_auc_score(y_test, y_test_score)
        print(f"ROC–AUC: {roc_auc:.4f}")
    except ValueError as e:
        print(f"ROC–AUC not computed: {e}")
else:
    print("ROC–AUC not available (no decision_function/predict_proba for this model).")



Fold 1 F1 Score: 0.8333
Fold 2 F1 Score: 0.8782
Fold 3 F1 Score: 0.8077
Fold 4 F1 Score: 0.8269
Fold 5 F1 Score: 0.7817

Final Evaluation on 20% Holdout Test Set
               precision    recall  f1-score   support

Non-offensive       0.90      0.82      0.86       100
    Offensive       0.83      0.91      0.87        96

     accuracy                           0.86       196
    macro avg       0.86      0.86      0.86       196
 weighted avg       0.87      0.86      0.86       196


